{"posts":[{"title":"Welcome to my blog!","text":"Welcome to my blog! Here is ZiwuValley’s blog. Find more details about me here. I will post some personal notes about Theoretical Computer Science &amp; Quantum Computing here. Click the button below for my writing plan. Ongoing: A summary of $\\mathbf{QIP= PSPACE}$ Notes about Abstract Algebra Notes about Complexity Theory Presumably appear in the future: Notes about Convex Optimization Notes about Multi-variable Calculus Notes about Representation Theory Notes about Matrix Analysis","link":"/Introduction/Introduction/"},{"title":"A Summary of $\\mathbf{QIP}&#x3D;\\mathbf{PSPACE}$","text":"This article notes the famous paper QIP=PSPACE1 written by Jain, Ji, UPADHYAY and Watrous. This semester I have a course named Introduction to Theoretical Computer Science requiring students to pick a conference paper about TCS and summarize it. Hence I write this article for this project. 1. BackgroundLet’s begin with $\\mathbf{P}$ and $\\mathbf{NP}$, the two widely known complexity classes. Suppose you are trying to solve some decision problems. You can solve some of them independently whereas you can’t solve other problems independently. In the second case, you may find an answer on the Internet(e.g. StackExchange) and convince yourself that the answer is reliable. Thus we can intuitively define the two complexity classes. $\\mathbf{P}$ represents a collection of decision problems that can be solved efficiently(i.e. in polynomial time) and $\\mathbf{NP}$ represents a collection of decision problems that you can efficiently verify the answer given by someone. Nevertheless, here comes a problem you can’t even efficiently verify the answer given by the prover. In this case, you have to keep asking the prover some questions randomly. What’s more, some provers may be dishonest, i.e. they may cheat on you. As a verifier, you need to consider the completeness and soundness of your interactive proof system, that is, accepting a yes-instance with high probability and rejecting a no-instance with low probability, respectively. Thus we can define the complexity class $\\mathbf{IP}$ as a collection of decision problems that have an interactive proof system. This concept was introduced by Babai2 and Goldwasser3 et al.. It’s trivial to see that $\\mathbf{P}\\subseteq\\mathbf{NP}\\subseteq\\mathbf{IP}$, but what is the upper bound of the computational model $\\mathbf{IP}$? Feldman4 proved the containment $\\mathbf{IP}\\subseteq \\mathbf{PSPACE}$ and Shamir5 proved the reverse containment $\\mathbf{PSPACE}\\subseteq \\mathbf{IP}$. Hence we have equality $\\mathbf{IP}=\\mathbf{PSPACE}$. In other words, the decision problems in $\\mathbf{IP}$ can be solved in polynomial space instead of time. The discussion above is only based on classically computational models. What if the verifier can use quantum circuits? Moreover, what if the prover and the verifier can send qubits to each other? Watrous6 first introduced the complexity class $\\mathbf{QIP}$ which is a quantum analog of $\\mathbf{IP}$ and proved that $\\mathbf{PSPACE}$ has constant-round quantum interactive proof systems which straightforward leads to $\\mathbf{IP}=\\mathbf{PSPACE}\\subseteq\\mathbf{QIP}$. Furthermore, Marriott and Watrous7 found a very simple form of quantum interactive proof systems, named quantum Arthur-Merlin games. That will be useful for us to analyze quantum interactive proof systems. Before Jain1 et al. proved $\\mathbf{QIP}\\subseteq\\mathbf{PSPACE}$, the best upper bound $\\mathbf{QIP}\\subseteq\\mathbf{EXP}$ had been proved by Kitaev and Watrous8 through the use of semidefinite programming, which will be used to prove $\\mathbf{QIP}\\subseteq\\mathbf{PSPACE}$ as well. Therefore Jain1 et al. proved $\\mathbf{QIP}\\subseteq\\mathbf{PSPACE}$, which implies $\\mathbf{QIP}=\\mathbf{PSPACE}=\\mathbf{IP}$. Hence introducing quantum resources cannot improve the computational ability of interactive proof systems. 2. PreliminariesThis section will strictly define some concepts. All the content of this section is from the original paper. 2.1 Linear AlgebraWe won’t talk about a bunch of basic concepts of Linear Algebra here. Let scripted letters such as $\\mathcal{V,X,Y}$ denote finite-dimensional complex vector spaces taking the form $\\mathbb{C}^{\\Sigma}\\cong \\mathbb{C}^{N}$ for some finite nonempty index set $|\\Sigma|=N$. The space of all linear operators from $\\mathcal{V}$ to $\\mathcal{V}$ is denoted $\\mathrm{L}(\\mathcal{V})$. Generally, we regard $\\Sigma$ as an orthonormal basis for $\\mathcal{V}$, so for a linear operator $A\\in\\mathrm{L}$, the rows and columns of the matrix representation of $A$ are indexed by $\\Sigma$. The identity operator from $\\mathcal{V}$ to $\\mathcal{V}$ is denoted by $I_{\\mathcal{V}}$. The inner product on $\\mathrm{L}(\\mathcal{V})$ is defined as \\langle A,B\\rangle\\colon=\\operatorname{Tr}(A^{\\dagger}B),A,B\\in\\mathrm{L}(\\mathcal{V}).The collection of all Hermitian operators from $\\mathcal{V}$ to $\\mathcal{V}$ is denoted $\\mathrm{Herm}(\\mathcal{V})$. The collection of all positive semidefinite operators from $\\mathcal{V}$ to $\\mathcal{V}$ is denoted $\\mathrm{Pos}(\\mathcal{V})$. The collection of all density operators(i.e. a positive semidefinite operator having unit trace) is denoted $\\mathrm{D}(\\mathcal{V})$. We say two operators $A\\le B$ iff. $A-B\\in\\mathrm{Pos}(\\mathcal{V})$. The eigenvalues of a Hermitian operator $H$ are all real, thus we can denote its maximal eigenvalue and minimal eigenvalue by $\\lambda_{\\max}(H)$ and $\\lambda_{\\min}(H)$, respectively. The spectral norm of an operator $A\\in\\mathrm{L}(\\mathcal{V})$ is defined as \\| A\\|\\colon=\\max\\{\\|Au\\|:u\\in\\mathcal{V},\\|u\\|=1\\}=\\sqrt{\\lambda_{\\max}(A^{\\dagger}A)}.The negative eigenspace of a Hermitian operator $A\\in\\mathrm{Herm}(\\mathcal{V})$ is spanned by all vector $u\\in\\mathcal{V}$ that is an eigenvector of $A$ having an associated eigenvalue that is negative. The exponential of an operator $A\\in\\mathrm{L}(\\mathcal{V})$ is defined as \\exp(A)\\colon=\\sum_{n=0}^{+\\infty}\\frac{A^n}{n!}.2.2 Quantum InformationIt’s unnecessary to clarify what Quantum Information is in light of proving $\\mathbf{QIP}=\\mathbf{PSPACE}$. We only need to know some basic concepts. A quantum register is a finite and nonempty sequence of qubits. We use sans serif font(e.g. $\\mathsf{X},\\mathsf{Y}$) to denote a quantum register. When a quantum register $\\mathsf{X}$ consists of $m$ qubits we say that $\\mathsf{X}$ is an $m$-qubit register, and in this case it has an associated vector space $\\mathcal{X}=\\mathbb{C}^{\\{0,1\\}^{m}}$. Moreover, its possible quantum states correspond to $\\mathrm{D}(\\mathcal{X})$. We use lower-case Greek letters like $\\rho,\\sigma$ to represent a quantum state. A measurement on $\\mathsf{X}$ is a finite collection of positive semidefinite operators $\\{P_b:b\\in\\Gamma\\}$ satisfying \\sum_{b\\in\\Gamma}P_b=I_{\\mathcal{X}}.The set $\\Gamma$ is the set of measurement outcomes. For any outcome $b\\in\\Gamma$, it occurs with probability $\\langle P_b,\\rho \\rangle$ where the quantum state of $\\mathsf{X}$ is $\\rho$. We can compose two or more registers and denote them by tensor product $\\otimes$. For registers $\\mathsf{X}$ and $\\mathsf{Y}$ having associated spaces $\\mathcal{X}$ and $\\mathcal{Y}$, we have that the space associated with the pair $(\\mathsf{X},\\mathsf{Y})$ is $\\mathcal{X}\\otimes\\mathcal{Y}$. If $\\mathsf{X}$ and $\\mathsf{Y}$ are prepared independently in the quantum states $\\rho\\in\\mathrm{D}(\\mathcal{X})$ and $\\sigma\\in\\mathrm{D}(\\mathcal{Y})$, then the state of $(\\mathsf{X},\\mathsf{Y})$ is $\\rho\\otimes\\sigma\\in\\mathrm{D}(\\mathcal{X}\\otimes\\mathcal{Y})$. But some $\\zeta\\in\\mathrm{D}(\\mathcal{X}\\otimes\\mathcal{Y})$ may not be separable, and in that case, we say $\\mathsf{X}$ and $\\mathsf{Y}$ are in entanglement. Entanglement is one of the most important and distinguished features of Quantum Information. We also want to extract $\\mathsf{X}$’s information from the pair $(\\mathsf{X},\\mathsf{Y})$. There is a uniquely determined reduced state of $\\mathsf{X}$. In mathematical terms, the reduced state is defined by the partial trace over $\\mathcal{Y}$, which is a linear mapping $\\operatorname{Tr}_{\\mathcal{Y}}$ from $\\mathrm{L}(\\mathcal{X}\\otimes\\mathcal{Y})$ to $\\mathrm{L}(\\mathcal{X})$ satisfying $\\operatorname{Tr}_{\\mathcal{Y}}(A\\otimes B)=\\operatorname{Tr}(B)A$ for every $A\\in\\mathrm{L}(\\mathcal{X})$ and $B\\in\\mathrm{L}(\\mathcal{Y})$. Thus we have $\\langle A,\\operatorname{Tr}_{\\mathcal{Y}}(B)\\rangle=\\langle A\\otimes I_{\\mathcal{Y}},B\\rangle$ for every $A\\in\\mathrm{L}(\\mathcal{X})$ and $B\\in\\mathrm{L}(\\mathcal{X}\\otimes\\mathcal{Y})$. Transformations of quantum registers are vital as well. A transformation of a quantum register can be described by a unitary operator $U$. If a register $\\mathsf{X}$ is in a state $\\rho\\in\\mathrm{D}(\\mathcal{X})$, and a transformation described by a unitary operator $U\\in\\mathrm{L}(\\mathcal{X})$ is performed on this register, its state becomes $U\\rho U^{\\dagger}$. 2.3 Quantum Interactive Proof Systems and Single-Coin Arthur-Merlin GamesWe have known quantum interactive proof systems, however, it’s too complicated to analyze. Luckily, Marriot and Watrous7 proved that it is always possible to transform a given quantum interactive proof system into a Quantum Merlin-Arthur-Merlin game, which is conceptually simpler. Let $p$ denote a polynomial, which is independent of the input and fixed, and $x$ denote the input string. In each round, the prover can send an $m$-qubit register to the verifier, for $m=p(n)$, where $n=\\left|x \\right|$ is the length of the input string. Note that it’s not necessary to restrict all registers to have the same length. Therefore the transformed proof system has the following form. The prover sends an $m$-qubit register $\\mathsf{X}$ to the verifier and the verifier cannot interact with this register. The verifier generates a bit $a\\in\\{0,1\\}$ uniformly at random and sends it to the prover. The prover sends an $m$-qubit register $\\mathsf{Y}$ to the verifier. The verifier measures the pair $(\\mathsf{X},\\mathsf{Y})$ by one of two binary-outcome measurements: $\\{\\Pi_{0}^0,\\Pi_{1}^{0} \\}$ in case $a=0$ and $\\{\\Pi_{0}^{1},\\Pi_{1}^{1} \\}$ in case $a=1$. The verifier accepts if the outcome is $1$ otherwise it rejects. Note that the measurements should be implemented efficiently, that is, applying a polynomial number of quantum gates(say, a certain universal set of quantum gates) to the $2m$ qubits. Marriot and Watrous7 showed that its completeness is perfect and its soundness error is bounded by $1/2+\\varepsilon$ for any desired constant $\\varepsilon&gt;0$. 2.4 Bounded-Depth Circuits and the Complexity ClassesTo evaluate the computational ability of parallel computers, theorists define $\\mathbf{NC}$ as a collection of functions that can be computed in polylogarithmic time by a parallel computer with a polynomial number of processors and $\\mathbf{NC}(poly)$ as a collection of functions that can be computed in polynomial time by a parallel computer with an exponential number of processors. The proof will make use of the fact that $\\mathbf{NC}(poly)=\\mathbf{PSPACE}$, which came from a more general result of Borodin9. Another fact of these complexity classes is they compose well. Specifically, if a decision problem $F\\in\\mathbf{NC}(poly)$ and another decision problem $G\\in\\mathbf{NC}$, then $G\\circ F\\in\\mathbf{NC}(poly)$. 2.5 Semidefinite ProgramsIn fact, we don’t need to know too much about semidefinite programs. We only need to know we can iteratively shrink the bound of the optimum of it and its dual problem by Matrix Multiplicative Weight Algorithm, which has an $\\mathbf{NC}(poly)$ implementation. 3. The Main IdeaAfter the tedious preparation, the main idea comes. Let’s check it out. 3.1 Semidefinite Programs for Single-Coin Quantum Arthur-Merlin GamesRecall that there is a transformed proof system that accepts a yes-instance with probability $1$ whereas it accepts a no-instance with probability $1/2+\\epsilon$ for any desired constant $\\epsilon&gt;0$. Hence if we find a strategy that makes the verifier accept with a probability greater than $1/2+\\epsilon$ for the prover, we can determine that the input is a no-instance otherwise it is a yes-instance. It can be naturally described as an optimization problem, more specifically, a semidefinite program. Depending on the verifier’s random bit $a$, the state of the pair $(\\mathsf{X},\\mathsf{Y})$ may be $\\rho_0,\\rho_1\\in\\mathrm{D}(\\mathcal{X},\\mathcal{Y})$. Since the prover cannot do anything to the register $\\mathsf{X}$ after he/she knows $a$, we have a constraint $\\operatorname{Tr}_{\\mathcal{Y}}(\\rho_0)=\\operatorname{Tr}_{\\mathcal{Y}}(\\rho_1)$. Therefore we are going to describe the probability that the verifier accepts. Suppose that the verifier’s measurements are $\\{\\Pi_{0}^{0},\\Pi_{1}^{0}\\}$ and $\\{\\Pi_{0}^{1},\\Pi_{1}^{1}\\}$, thus, the probability that the verifier accepts is \\frac{1}{2}\\langle \\Pi_{1}^{0},\\rho_0\\rangle+\\frac{1}{2}\\langle\\Pi_{1}^{1},\\rho_1\\rangle.Hence, the optimization problem is to maximize this probability. However, the constraint here is too tough to deal with. We need to relax this constraint. An optimization problem with the same optimum $u$ is to maximize the quantity \\frac{1}{2}\\langle\\Pi_{1}^{0},Q_0\\rangle+\\frac{1}{2}\\langle\\Pi_{1}^{1},Q_1\\rangle,over all operators $Q_0,Q_1\\in\\mathrm{Pos}(\\mathcal{X}\\otimes\\mathcal{Y}),\\xi\\in\\mathrm{D}(\\mathcal{X})$ such that $\\operatorname{Tr}_{\\mathcal{Y}}(Q_0)\\le\\xi$ and $\\operatorname{Tr}_{\\mathcal{Y}}(Q_1)\\le \\xi$. Moreover, for a positive number $\\alpha$, define two operators $P_0=\\Pi_{1}^{0}+\\alpha\\Pi_{0}^{0},P_1=\\Pi_{1}^{1}+\\alpha\\Pi_{0}^{1}$. They are semidefinite positive and invertible since $P_{0}^{-1}=\\Pi_{1}^{0}+\\Pi_{0}^{0}/\\alpha,P_{1}^{-1}=\\Pi_{1}^{1}+\\Pi_{0}^{1}/\\alpha$. We have the maximum value of the quantity \\frac{1}{2}\\langle P_{0}^{-2},Q_0\\rangle+\\frac{1}{2}\\langle P_{1}^{-2},Q_1\\rangleis in $[u,u+1/\\alpha^2]$. Since $\\langle P_{0}^{-2},Q_0\\rangle=\\operatorname{Tr}(P_{0}^{-2}Q_0)=\\operatorname{Tr}(P_{0}^{-1}Q_0P_{0}^{-1})$ and the mapping $T:Q_0\\mapsto P_{0}^{-1}Q_0P_{0}^{-1}$ is bijective, by change-of-variables, we get a simpler form of this optimization problem with its dual problem: \\begin{aligned} &\\quad\\underline{\\text{Primal Problem}}\\\\ \\max&\\quad\\operatorname{Tr}\\bigg(\\frac{1}{2}Q_0+\\frac{1}{2}Q_1\\bigg)\\\\ s.t.&\\quad\\operatorname{Tr}_{\\mathcal{Y}}(P_0Q_0P_0)\\le\\xi,\\\\ &\\quad\\operatorname{Tr}_{\\mathcal{Y}}(P_1Q_1P_1)\\le\\xi,\\\\ &\\quad Q_0,Q_1\\in\\mathrm{Pos}(\\mathcal{X}\\otimes\\mathcal{Y}),\\\\ &\\quad\\xi\\in\\mathrm{D}(\\mathcal{X}). \\end{aligned} \\qquad\\qquad\\qquad\\qquad\\qquad \\begin{aligned} &\\quad\\underline{\\text{Dual Problem}}\\\\ \\min&\\quad\\|\\frac{1}{2}R_0+\\frac{1}{2}R_1 \\|\\\\ s.t.&\\quad P_0(R_0\\otimes I_{\\mathcal{Y}})P_0\\ge I_{\\mathcal{X}}\\otimes I_{\\mathcal{Y}},\\\\ &\\quad P_1(R_1\\otimes I_{\\mathcal{Y}})P_1\\ge I_{\\mathcal{X}}\\otimes I_{\\mathcal{Y}},\\\\ &\\quad R_0,R_1\\in\\mathrm{Pos}(\\mathcal{X}). \\end{aligned}For every $(Q_0,Q_1,\\xi)$ and $(R_0,R_1)$ obtainable, the weak duality holds, that is \\begin{aligned} \\operatorname{Tr}\\bigg{(}\\frac{1}{2}Q_0+\\frac{1}{2}Q_1 \\bigg{)}&= \\frac{1}{2}\\langle I_{\\mathcal{X}}\\otimes I_{\\mathcal{Y}},Q_0 \\rangle+\\frac{1}{2}\\langle I_{\\mathcal{X}}\\otimes I_{\\mathcal{Y}},Q_1 \\rangle\\\\ &\\le \\frac{1}{2}\\langle P_0(R_0\\otimes I_{\\mathcal{Y}})P_0,Q_0\\rangle+\\frac{1}{2}\\langle P_{1}(R_1\\otimes I_{\\mathcal{Y}})P_1,Q_1\\rangle\\\\ &=\\frac{1}{2}\\langle R_0,\\operatorname{Tr}_{\\mathcal{Y}}(P_0Q_0P_0)\\rangle+\\frac{1}{2}\\langle R_1,\\operatorname{Tr}_{\\mathcal{Y}}(P_1Q_1P_1)\\rangle\\\\ &\\le \\frac{1}{2}\\langle R_0,\\xi\\rangle+\\frac{1}{2}\\langle R_1,\\xi\\rangle\\\\ &=\\langle\\frac{1}{2}R_0+\\frac{1}{2}R_1,\\xi\\rangle\\\\ &\\le \\|\\frac{1}{2}R_0+\\frac{1}{2}R_1 \\|. \\end{aligned}The last inequality can be proved by the spectral decomposition of $\\xi$. 3.2 Matrix Multiplicative Weight Algorithm for Semidefinite ProgramsMatrix multiplicative weight algorithm is usually used to approximately solve semidefinite programming. Here we need to slightly modify it so that it can be suitable for our problems. First, we set some values M=2^m,\\quad\\alpha=4,\\quad\\gamma=\\frac{4}{3},\\quad \\varepsilon=\\frac{1}{64},\\quad\\text{and}\\quad \\delta=\\frac{\\varepsilon}{\\alpha^2 }.When the optimal value of this semidefinite program is at least $1$, we can determine that the verifier will accept. When the optimal value of it is at most $1/2+\\varepsilon+1/\\alpha^2&lt;5/8$, we can determine that the verifier will reject. Jain et al. used the matrix multiplicative weight algorithm to solve this problem. The algorithm can be described as follows. Compute P_a=\\Pi_{1}^{a}+\\alpha\\Pi_{0}^{a},\\quad X_{a}^{(0)}=I_{\\mathcal{X}\\otimes\\mathcal{Y}},\\quad\\text{and}\\quad \\rho_{a}^{(0)}=\\frac{X_{a}^{(0)}}{\\operatorname{Tr}(X_{0}^{(0)}+X_{1}^{(0)})}for $a\\in\\{0,1\\}$, and compute Y_{0}^{(0)}=I_{\\mathcal{X}},\\quad \\sigma^{(0)}=\\frac{Y^{(0)}}{\\operatorname{Tr}(Y^{(0)})},\\quad\\text{and}\\quad T=\\lceil\\frac{4\\ln M}{\\varepsilon^{2}\\delta}\\rceil. Repeat the following steps for each $t$ from $0$ to $T-1$: Compute Z_{a}^{(t)}=\\frac{\\gamma}{2}\\sigma^{(t)}-\\operatorname{Tr}_{\\mathcal{Y}}(P_a\\rho_aP_a)and compute the projection $\\Delta_{a}^{(t)}$ onto the negative eigenspace of $Z_{a}^{(t)}$, for $a\\in\\{0,1\\}$. Compute \\beta^{(t)}=\\langle\\Delta_{0}^{(t)}\\otimes I_{\\mathcal{Y}},P_0\\rho_0^{(t)}P_0\\rangle+\\langle\\Delta_{1}^{(t)}\\otimes I_{\\mathcal{Y}},P_1\\rho_1^{(t)}P_1\\rangle.If $\\beta^{(t)}\\le\\varepsilon$, accept and exit. We claim that $(2\\rho_{0}^{(t)},2\\rho_{1}^{(t)},\\sigma^{(t)})$ is close to a feasible solution to the primal problem. Compute \\begin{aligned} X_{a}^{(t+1)}&=\\exp\\bigg{(}-\\sum_{j=0}^{t}\\frac{\\varepsilon\\delta}{\\beta^{(j)}}P_a(\\Delta_{a}^{(j)}\\otimes I_{\\mathcal{Y}})P_a \\bigg{)}\\\\ \\rho_{a}^{(t+1)}&=\\frac{X_{a}^{(t+1)}}{\\operatorname{Tr}(X_{0}^{(t+1)}+X_{1}^{(t+1)})} \\end{aligned}for $a\\in\\{0,1\\}$ and \\begin{aligned} Y^{(t+1)}&=\\exp\\bigg{(}\\sum_{j=0}^{t}\\frac{\\varepsilon^2}{2\\beta^{(j)}}(\\Delta_0^{(j)}+\\Delta_1^{(j)}) \\bigg{)}\\\\ \\sigma^{(t+1)}&=\\frac{Y^{(t+1)}}{\\operatorname{Tr}(Y^{(t+1)})}. \\end{aligned} Reject. We claim that $(\\sum_{t=0}^{T-1}\\Delta_{0}^{(t)}/\\beta^{(t)},\\sum_{t=0}^{T-1}\\Delta_1^{(t)}/\\beta^{(t)})$ is close to a feasible solution to the dual problem. 4. Analysis4.1 ComplexityWe claim that this algorithm has an $\\mathbf{NC}(poly)$ implementation. Here we won’t dive into those details, which you can find in the original paper. In addition, I can’t find some references in the original paper, so I won’t mention them. First, it is important to stress that those problems, which are sums, products, tensor products, inverses, and iterated sums and products of matrices, are known to be in $\\mathbf{NC}$. Therefore, those measurements of the verifier can be computed in $\\mathbf{NC}(poly)$ since they can be decomposed into a polynomial number of quantum gates. We can compute all the matrices parallelly in $\\mathbf{NC}(poly)$ and then compute their product in $\\mathbf{NC}$. Finally, we can approximately compute the exponential of a given matrix and the projection onto the negative eigenspace of a given matrix in $\\mathbf{NC}$. Given an $n\\times n$ matrix $A$ satisfying $|A|\\le k$ with a given integer, we can directly truncate the series \\exp(A)=\\sum_{k\\ge 0}\\frac{A^k}{k!}to a number of terms linear in $k+\\ln(1/\\varepsilon)$ with the error bound $\\varepsilon$. We can also approximately compute roots of integer polynomials10 and characteristic polynomials of matrices11 in $\\mathbf{NC}$. It provides us with an $\\mathbf{NC}$ algorithm to approximately compute the negative eigenspace of a given matrix. After all, we implement this algorithm in $\\mathbf{NC}(poly)=\\mathbf{PSPACE}$. 4.2 Accuracy4.2.0 Some Useful Theorems and LemmasTo better analyze the accuracy, we need to use some theorems and lemmas. THEOREM 1(Golden-Thompson Inequality). For any Hermitian operators $A,B\\in\\mathrm{Herm}(\\mathcal{V})$, the inequality \\operatorname{Tr}[\\exp(A+B)]\\le \\operatorname{Tr}[\\exp(A)\\exp(B)]holds. The proof can be found in Matrix Analysis12. LEMMA 1. Let $R\\in\\mathrm{Pos}(\\mathcal{V})$ be a positive semidefinite operator satisfying $R\\le I_{\\mathcal{V}}$. For every real number $\\eta$, it holds that \\exp(\\eta R)\\le I_{\\mathcal{V}}+\\eta\\exp(\\eta)RPROOF. Consider each term in the spectral decomposition of $R$ separately. Then we can decompose $I_{\\mathcal{V}}$ into the projections onto the eigenspaces of $R$. Hence we only need to prove that \\exp(\\eta\\lambda)\\le 1+\\eta\\exp(\\eta)\\lambda,\\qquad\\qquad\\forall\\lambda\\in[0,1].The case $\\lambda=0$ is trivial. For $\\lambda\\in(0,1]$, by the Mean Value Theorem, we have that \\frac{\\exp(\\eta\\lambda)-1}{\\lambda}=\\eta\\exp(\\eta\\lambda_0)\\le \\eta\\exp\\eta,\\qquad\\text{for some }\\lambda_0\\in[0,1].LEMMA 2(Bounds for Matrix Multiplicative Weight Algorithm). Let $T$ be a positive integer, $M^{(0)},M^{(1)},\\cdots,M^{(t-1)}\\in\\mathrm{Pos}(\\mathcal{V})$ be operators on a space $\\mathcal{V}=\\mathbb{C}^{N}$ satisfying $0\\le M^{(t)}\\le I_{\\mathcal{V}}$ for each $t\\in\\{0,\\cdots,T-1 \\}$, and $\\varepsilon&gt;0$ be a positive number. Then the two inequalities hold: For X^{(0)}=I_{\\mathcal{V}},\\quad X^{(t+1)}=\\exp\\bigg{(}-\\varepsilon\\sum_{k=0}^{t}M^{(k)} \\bigg{)},\\quad \\text{and}\\quad \\rho^{(t)}=\\frac{X^{(t)}}{\\operatorname{Tr}(X^{(t)})}for each $t\\in\\{0,\\dots,T-1 \\}$, we have that \\lambda_{\\min}\\bigg{(}\\sum_{t=0}^{T-1}M^{(t)} \\bigg{)}\\ge \\exp(-\\varepsilon)\\sum_{t=0}^{T-1}\\bigg\\langle\\rho^{(t)},M^{(t)} \\bigg\\rangle-\\frac{\\ln N}{\\varepsilon}. For Y^{(0)}=I_{\\mathcal{V}},\\quad Y^{(t+1)}=\\exp\\bigg{(}\\varepsilon\\sum_{k=0}^{t}M^{(k)} \\bigg{)},\\quad \\text{and}\\quad \\sigma^{(t)}=\\frac{Y^{(t)}}{\\operatorname{Tr}(Y^{(t)})}for each $t\\in\\{0,\\cdots,T-1\\}$, we have that \\lambda_{\\max}\\bigg{(}\\sum_{t=0}^{T-1}M^{(t)} \\bigg{)}\\le \\exp(\\varepsilon)\\sum_{t=0}^{T-1}\\bigg\\langle\\sigma^{(t)},M^{(t)}\\bigg\\rangle+\\frac{\\ln N}{\\varepsilon}. PROOF. We prove the first inequality and readers can prove the second one similarly. The main idea is to consider the trace of the sum of $M^{(t)}$ as a bridge between the two sides of the inequality. More specifically, since the operator $X^{(T)}$ is positive semidefinite, we have \\operatorname{Tr}\\big(X^{(T)}\\big)\\ge \\lambda_{\\min}\\big(X^{(T)}\\big)=\\lambda_{\\min}\\bigg{(}\\exp\\bigg(-\\varepsilon\\sum_{t=0}^{T-1}M^{(k)} \\bigg)\\bigg{)}=\\exp\\bigg(-\\varepsilon\\lambda_{\\min}\\bigg(\\sum_{t=0}^{T-1}M^{(k)} \\bigg)\\bigg).Then we apply Golden-Thompson inequality to evaluate $\\operatorname{Tr}(X^{(T)})$, so for any $t\\in\\{0,\\cdots,T-1\\}$ we have \\begin{aligned} \\operatorname{Tr}\\big(X^{(t+1)}\\big)&\\le\\operatorname{Tr}\\big(X^{(t)}\\exp(-\\varepsilon M^{(t)})\\big). & \\end{aligned}According to LEMMA 1, we have $\\exp(-\\varepsilon M^{(t)})\\le I_{\\mathcal{V}}-\\varepsilon\\exp(-\\varepsilon)M^{(t)}$. Therefore, we conclude that \\begin{aligned} \\operatorname{Tr}\\big(X^{(t+1)}\\big)&\\le \\operatorname{Tr}\\big(X^{(t)}-\\varepsilon\\exp(-\\varepsilon)X^{(t)}M^{(t)}\\big)\\\\ &=\\operatorname{Tr}\\big(X^{(t)}\\big)\\bigg{(}1-\\varepsilon\\exp(-\\varepsilon)\\operatorname{Tr}\\big(\\frac{X^{(t)}}{\\operatorname{Tr}\\big(X^{(t)}\\big)}M^{(t)}\\big) \\bigg{)}\\\\ &=\\operatorname{Tr}\\big(X^{(t)}\\big)\\bigg{(}1-\\varepsilon\\exp(-\\varepsilon)\\big\\langle\\rho^{(t)},M^{(t)}\\big\\rangle\\bigg{)}\\\\ &\\le \\operatorname{Tr}\\big(X^{(t)} \\big)\\exp\\bigg(-\\varepsilon\\exp(-\\varepsilon)\\big\\langle\\rho^{(t)},M^{(t)}\\big\\rangle \\bigg). \\end{aligned}Apply this inequality recursively, thus we deduce that \\begin{aligned} \\operatorname{Tr}\\big(X^{(T)} \\big)&\\le\\operatorname{Tr}\\big(X^{(0)}\\big)\\prod_{t=0}^{T-1}\\exp\\bigg(-\\varepsilon\\exp(-\\varepsilon)\\big\\langle\\rho^{(t)},M^{(t)}\\big\\rangle \\bigg)\\\\ &=N\\exp\\bigg(-\\varepsilon\\exp(-\\varepsilon)\\sum_{t=0}^{T-1}\\big\\langle\\rho^{(t)},M^{(t)}\\big\\rangle \\bigg). \\end{aligned}Finally, we combine these two inequalities so we have \\begin{aligned} & \\exp\\bigg(-\\varepsilon\\lambda_{\\min}\\bigg(\\sum_{t=0}^{T-1}M^{(k)} \\bigg)\\bigg)\\le N\\exp\\bigg(-\\varepsilon\\exp(-\\varepsilon)\\sum_{t=0}^{T-1}\\big\\langle\\rho^{(t)},M^{(t)}\\big\\rangle \\bigg)\\\\ \\Longleftrightarrow\\quad& -\\varepsilon\\lambda_{\\min}\\bigg(\\sum_{t=0}^{T-1}M^{(k)} \\bigg)\\le \\ln N-\\varepsilon\\exp(-\\varepsilon)\\sum_{t=0}^{T-1}\\big\\langle\\rho^{(t)},M^{(t)}\\big\\rangle\\\\ \\Longleftrightarrow\\quad& \\lambda_{\\min}\\bigg(\\sum_{t=0}^{T-1}M^{(k)} \\bigg)\\ge \\exp(-\\varepsilon)\\sum_{t=0}^{T-1}\\big\\langle\\rho^{(t)},M^{(t)}\\big\\rangle-\\frac{\\ln N}{\\varepsilon} \\end{aligned} 4.2.1 Ignore the ErrorWe first analyze the accuracy when those computations such as exponentials and projections onto negative eigenspaces of matrices can be solved precisely. Case 1. AcceptSuppose that the algorithm accepts the input. For convenience, here we omit the superscript. Define Q_a=\\frac{2\\rho_a}{\\gamma+2\\beta}\\in\\mathrm{Pos}(\\mathcal{X}\\otimes \\mathcal{Y})for $a\\in\\{0,1\\}$, and define \\xi=\\frac{\\gamma\\sigma+2\\Delta_0\\operatorname{Tr}_{\\mathcal{Y}}(P_0\\rho_0P_0)\\Delta_0+2\\Delta_1\\operatorname{Tr}_{\\mathcal{Y}}(P_1\\rho_1P_1)\\Delta_1}{\\gamma+2\\beta}\\in\\mathrm{D}(\\mathcal{X}).For each $a\\in\\{0,1\\}$, since $\\Delta_a\\sigma\\Delta_a$ is positive semidefinite, it holds that -\\Delta_a\\operatorname{Tr}_{\\mathcal{Y}}(P_a\\rho_a P_a)\\Delta_a\\le \\Delta_a\\bigg(\\frac{\\gamma\\sigma}{2}-\\operatorname{Tr}_{\\mathcal{Y}}(P_a\\rho_aP_a) \\bigg)\\Delta_a.According to the definition of $\\Delta_a$, we have that \\begin{aligned} \\bigg(\\frac{\\gamma\\sigma}{2}-\\operatorname{Tr}_{\\mathcal{Y}}(P_a\\rho_aP_a) \\bigg)(I_{\\mathcal{X}}-\\Delta_a)\\ge 0. \\end{aligned}It implies that \\Delta_a\\bigg(\\frac{\\gamma\\sigma}{2}-\\operatorname{Tr}_{\\mathcal{Y}}(P_a\\rho_aP_a) \\bigg)\\Delta_a=\\bigg(\\frac{\\gamma\\sigma}{2}- \\operatorname{Tr}_{\\mathcal{Y}}(P_a\\rho_aP_a)\\bigg)\\Delta_a\\le \\frac{\\gamma\\sigma}{2}-\\operatorname{Tr}_{\\mathcal{Y}}(P_a\\rho_aP_a).Hence it holds that \\operatorname{Tr}_{\\mathcal{Y}}(P_a\\rho_aP_a)\\le \\frac{\\gamma\\sigma}{2}+\\Delta_a\\operatorname{Tr}_{\\mathcal{Y}}(P_a\\rho_aP_a)\\Delta_a,followed by \\operatorname{Tr}_{\\mathcal{Y}}(P_aQ_aP_a)\\le\\frac{\\gamma\\sigma+2\\Delta_a\\operatorname{Tr}_{\\mathcal{Y}}(P_a\\rho_aP_a)\\Delta_a}{\\gamma+2\\beta}\\le \\xi.So $(Q_0,Q_1,\\xi)$ is a feasible solution to the primal problem. Moreover, since $\\beta\\le \\varepsilon$, we deduce that \\operatorname{Tr}\\bigg(\\frac{1}{2}Q_0+\\frac{1}{2}Q_1 \\bigg)=\\frac{\\operatorname{Tr}(\\rho_0+\\rho_1)}{\\gamma+2\\beta}=\\frac{1}{\\gamma+2\\beta}>\\frac{5}{8}.Hence we can determine the input string is a yes-instance. Case 2. RejectFor each $a\\in\\{0,1\\}$, define R_a=\\frac{1+4\\varepsilon}{T}\\sum_{t=0}^{T-1}\\frac{\\Delta_a^{(t)}}{\\beta^{(t)}}.Once we prove the smallest eigenvalue of $P_a(R_a\\otimes I_{\\mathcal{Y}})P_a$ is not smaller than $1$, we know that $(R_0,R_1)$ is a feasible solution to the dual problem. It inspires us to apply the first inequality in LEMMA 2. We combine $R_0$ and $R_1$ by defining M^{(t)}=\\frac{\\delta}{\\beta^{(t)}}\\begin{bmatrix} P_0(\\Delta_0\\otimes I_{\\mathcal{Y}})P_0 & 0\\\\ 0 & P_1(\\Delta_1\\otimes I_{\\mathcal{Y}})P_1 \\end{bmatrix} ,\\quad X^{(t)}=\\begin{bmatrix} X_0^{(t)} & 0\\\\ 0 & X_1^{(t)} \\end{bmatrix} ,\\quad \\text{and}\\quad \\rho^{(t)}=\\begin{bmatrix} \\rho_{0}^{(t)} & 0\\\\ 0 & \\rho_{1}^{(t)} \\end{bmatrix}for each $t\\in\\{0,\\cdots,T-1\\}$. Since $\\beta^{(t)}\\ge \\varepsilon$, it’s easy to verify that $0\\le M^{(t)}\\le I$. According to LEMMA 2, we have that \\begin{aligned} \\lambda_{\\min}\\bigg(\\sum_{t=0}^{T-1}M^{(t)} \\bigg)&\\ge \\exp(-\\varepsilon)\\sum_{t=0}^{T}\\bigg\\langle\\rho^{(t)},M^{(t)}\\bigg\\rangle-\\frac{\\ln 2M^2}{\\varepsilon}\\\\ &=\\exp(-\\varepsilon)\\delta T-\\frac{\\ln 2M^2}{\\varepsilon}. \\end{aligned}Moreover, we conclude that \\begin{aligned} \\lambda_{\\min}\\begin{bmatrix} P_0(R_0\\otimes I_{\\mathcal{Y}})P_0 & 0\\\\ 0 & P_1(R_1\\otimes I_{\\mathcal{Y}})P_1 \\end{bmatrix} &\\ge (1+4\\varepsilon)\\bigg(\\exp(-\\varepsilon)-\\frac{\\ln 2M^2}{\\varepsilon\\delta T} \\bigg)\\\\ &\\ge (1+4\\varepsilon )\\bigg(1-\\varepsilon-\\frac{\\ln M^4}{\\varepsilon\\delta\\frac{4\\ln M}{\\varepsilon^2\\delta}} \\bigg)\\\\ &=(1+4\\varepsilon)(1-2\\varepsilon) \\ge 1. \\end{aligned}It is sufficient to deduce $P_a(\\Delta_a\\otimes I_{\\mathcal{V}})P_a\\ge I_{\\mathcal{X}}\\otimes I_{\\mathcal{Y}}$, so $(R_0,R_1)$ is a feasible solution to the dual problem.Then we evaluate the value of this solution. The proof will make use of the fact that the spectral norm of a Hermitian operator is its maximal eigenvalue. Define M^{(t)}=\\frac{\\varepsilon}{2\\beta^{(t)}}\\big(\\Delta_0^{(t)}+\\Delta_1^{(t)} \\big),and apply the second inequality in LEMMA 2, so that \\lambda_{\\max}\\bigg(\\sum_{t=0}^{T-1}M^{(t)} \\bigg)\\le \\exp(\\varepsilon)\\sum_{t=0}^{T-1}\\bigg\\langle\\sigma^{(t)},M^{(t)}\\bigg\\rangle+\\frac{\\ln M}{\\varepsilon}.Note that \\bigg(\\frac{\\gamma\\sigma^{(t)}}{2}-\\operatorname{Tr}_{\\mathcal{Y}}(P_a\\rho_a^{(t)}P_a) \\bigg)\\Delta_a^{(t)}\\le 0,so we have \\frac{\\gamma}{2}\\bigg\\langle\\sigma^{(t)},\\Delta_a^{(t)}\\bigg\\rangle\\le \\bigg\\langle\\operatorname{Tr}_{\\mathcal{Y}}(P_a\\rho_a^{(t)}P_a),\\Delta_a^{(t)}\\bigg\\rangle=\\bigg\\langle P_a\\rho_a^{(t)}P_a,\\Delta_a^{(t)}\\otimes I_{\\mathcal{Y}} \\bigg\\rangle,which implies that $\\langle \\sigma^{(t)},M^{(t)}\\rangle\\le \\varepsilon/\\gamma$. Thus we can obtain an upper-bound \\lambda_{\\max}\\bigg(\\sum_{t=0}^{T-1}M^{(t)} \\bigg)\\le \\frac{\\varepsilon\\exp(\\varepsilon)T}{\\gamma}+\\frac{\\ln M}{\\varepsilon}.Now we can evaluate the value of the solution, that is \\begin{aligned} \\|\\frac{1}{2}R_0+\\frac{1}{2}R_1\\|&=\\frac{1+4\\varepsilon}{\\varepsilon T}\\lambda_{\\max}\\bigg(\\sum_{t=0}^{T-1}M^{(t)} \\bigg)\\\\ &\\le (1+4\\varepsilon)\\bigg(\\frac{\\exp \\varepsilon}{\\gamma}+\\frac{\\ln M}{\\varepsilon^2\\frac{4\\ln M}{\\varepsilon^2\\delta}} \\bigg)\\\\ &=(1+4\\varepsilon)\\bigg(\\frac{\\exp\\varepsilon}{\\gamma}+\\frac{\\delta}{4} \\bigg)","link":"/Theoretical-Computer-Science/QIP-PSPACE/"}],"tags":[{"name":"Quantum Computing","slug":"Quantum-Computing","link":"/tags/Quantum-Computing/"},{"name":"Complexity Theory","slug":"Complexity-Theory","link":"/tags/Complexity-Theory/"}],"categories":[{"name":"Theoretical Computer Science","slug":"Theoretical-Computer-Science","link":"/categories/Theoretical-Computer-Science/"},{"name":"Introduction","slug":"Introduction","link":"/categories/Introduction/"}],"pages":[{"title":"ABOUT ME","text":"My name is Xunuo Wang. You can also call me Ryan Wang. I am a freshman at John Hopcroft Class, Zhiyuan College, Shanghai Jiao Tong University. You can reach me by dropping emails to ziwuvalley[AT]sjtu[DOT]edu[DOT]cn or sending messages to my twitter account. I am mainly interested in Theoretical Computer Science and Quantum Computing, especially the overlap of these two fields. Currently, I am focusing on these following topics: Quantum Complexity Theory Quantum Learning Theory Quantum Machine Learning for Combinatorial Optimization I will post my CV here when my experience become various.","link":"/about/index.html"}]}